{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-assignment4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNA+n81MWaHAfm5bR1VLUZp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HardikPrabhu/Natural-Language-Processing-on-the-Covid-19-Research-papers/blob/main/NLP_assignment4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KT8IDbC5BYQ"
      },
      "source": [
        "Hardik Prabhu\n",
        "\n",
        "Roll Number: MDS201912"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEKv-UYeTZ9-",
        "outputId": "2f052ec1-6a61-4484-b480-6e3d5bbd59da"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QytknVwv5zm8"
      },
      "source": [
        "**Importing the preprocessed text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpfkiGyU5GN6"
      },
      "source": [
        "location=\"/content/drive/MyDrive/pdf_json\"  #location of json files\n",
        "save_location=\"/content/drive/MyDrive\"      #location on my gdrive\n",
        "os.chdir(save_location)\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8kV5IPI6GGd"
      },
      "source": [
        "We have the preprocessed text as a list(corpus) of list(document) of list(sentence) of words. We create one string combinging all the pre-processed text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "44ncR_VxVKhz",
        "outputId": "340dbd47-74f7-494b-e19a-cb8358b39bf2"
      },
      "source": [
        "text=\"\"\n",
        "for m in range(16):\n",
        "  print(m)\n",
        "  with open(\"nlp-\"+str(m), \"rb\") as f:\n",
        "    corp=pickle.load(f)\n",
        "  for document in corp: \n",
        "   for sentence in document:\n",
        "    for word in sentence:\n",
        "        text = text + ' ' + word\n",
        "    text = text + '.' \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-85ee25703c97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nlp-\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mcorp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m    \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsPUeUY5owtO"
      },
      "source": [
        "It is taking too long to convert it one by one. After multiple timeout errors, I have decided to work on a subset of the large corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvvskmlvFNhP"
      },
      "source": [
        " with open(\"nlp-0\", \"rb\") as f:\n",
        "    corp=pickle.load(f)\n",
        "\n",
        "corp=corp[:550]\n",
        "# We have 80533 sentences as the model input.   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IEMkqulFa8-",
        "outputId": "81b8028b-6f27-46c0-ad7d-34c9eac939f9"
      },
      "source": [
        "text=\"\"\n",
        "i=0\n",
        "for document in corp:\n",
        "  print(i)\n",
        "  i=i+1 \n",
        "  for sentence in document:\n",
        "    for word in sentence:\n",
        "        text = text + ' ' + word\n",
        "    text = text + '.' \n",
        "\n",
        "f = open(\"text_nlp4.txt\", \"w\")\n",
        "f.write(text) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "400\n",
            "401\n",
            "402\n",
            "403\n",
            "404\n",
            "405\n",
            "406\n",
            "407\n",
            "408\n",
            "409\n",
            "410\n",
            "411\n",
            "412\n",
            "413\n",
            "414\n",
            "415\n",
            "416\n",
            "417\n",
            "418\n",
            "419\n",
            "420\n",
            "421\n",
            "422\n",
            "423\n",
            "424\n",
            "425\n",
            "426\n",
            "427\n",
            "428\n",
            "429\n",
            "430\n",
            "431\n",
            "432\n",
            "433\n",
            "434\n",
            "435\n",
            "436\n",
            "437\n",
            "438\n",
            "439\n",
            "440\n",
            "441\n",
            "442\n",
            "443\n",
            "444\n",
            "445\n",
            "446\n",
            "447\n",
            "448\n",
            "449\n",
            "450\n",
            "451\n",
            "452\n",
            "453\n",
            "454\n",
            "455\n",
            "456\n",
            "457\n",
            "458\n",
            "459\n",
            "460\n",
            "461\n",
            "462\n",
            "463\n",
            "464\n",
            "465\n",
            "466\n",
            "467\n",
            "468\n",
            "469\n",
            "470\n",
            "471\n",
            "472\n",
            "473\n",
            "474\n",
            "475\n",
            "476\n",
            "477\n",
            "478\n",
            "479\n",
            "480\n",
            "481\n",
            "482\n",
            "483\n",
            "484\n",
            "485\n",
            "486\n",
            "487\n",
            "488\n",
            "489\n",
            "490\n",
            "491\n",
            "492\n",
            "493\n",
            "494\n",
            "495\n",
            "496\n",
            "497\n",
            "498\n",
            "499\n",
            "500\n",
            "501\n",
            "502\n",
            "503\n",
            "504\n",
            "505\n",
            "506\n",
            "507\n",
            "508\n",
            "509\n",
            "510\n",
            "511\n",
            "512\n",
            "513\n",
            "514\n",
            "515\n",
            "516\n",
            "517\n",
            "518\n",
            "519\n",
            "520\n",
            "521\n",
            "522\n",
            "523\n",
            "524\n",
            "525\n",
            "526\n",
            "527\n",
            "528\n",
            "529\n",
            "530\n",
            "531\n",
            "532\n",
            "533\n",
            "534\n",
            "535\n",
            "536\n",
            "537\n",
            "538\n",
            "539\n",
            "540\n",
            "541\n",
            "542\n",
            "543\n",
            "544\n",
            "545\n",
            "546\n",
            "547\n",
            "548\n",
            "549\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13563724"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FFLOCvNv8Ej"
      },
      "source": [
        "f = open(\"text_nlp4.txt\", \"r\")\n",
        "text=f.read() \n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLeBPSP17R6R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a835ddb-345a-4285-ba5e-ff86e1323016"
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "166 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1L7HDuZpEyX"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWk7QmkX7aIy"
      },
      "source": [
        "ids_from_chars = preprocessing.StringLookup(\n",
        "    vocabulary=list(vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8scgZ3IcDMk6",
        "outputId": "fad46444-4e78-4616-8741-f789d4f672cf"
      },
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmg2WSYxDQO9",
        "outputId": "795701e5-4aad-4703-8296-0b257d6d1f85"
      },
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[4, 5, 6, 7, 8, 9, 10], [27, 28, 29]]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_in79WhDURY"
      },
      "source": [
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJqmF26ODYOk",
        "outputId": "2f918d0f-31c5-4c30-9383-3bdc694b2c7b"
      },
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SclBEC8JDc3N",
        "outputId": "8eda3e75-a3b2-4b0b-f56e-83fd3fa4196c"
      },
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gffo_2opDgT3"
      },
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnpUfegHDjyC",
        "outputId": "65572478-e106-4ead-c5ce-3cbe402903d9"
      },
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(13563724,), dtype=int64, numpy=array([ 2, 19, 18, ...,  2,  4,  3])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7s5REruDqPx"
      },
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_m4CjKWFDuZp",
        "outputId": "cff6d38d-0d8c-4a75-fef5-da6ba8a6d92b"
      },
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " \n",
            "p\n",
            "o\n",
            "s\n",
            "t\n",
            "w\n",
            "e\n",
            "a\n",
            "n\n",
            "i\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm-SzZNADzYb"
      },
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayqyVMt9D2-g",
        "outputId": "328b5aaf-ce4f-4084-95c4-25f36353bbc3"
      },
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b' ' b'p' b'o' b's' b't' b'w' b'e' b'a' b'n' b'i' b'n' b'g' b' ' b'm'\n",
            " b'o' b'r' b't' b'a' b'l' b'i' b't' b'y' b' ' b'i' b'n' b' ' b'c' b'o'\n",
            " b'm' b'm' b'e' b'r' b'c' b'i' b'a' b'l' b' ' b's' b'w' b'i' b'n' b'e'\n",
            " b' ' b'p' b'r' b'o' b'd' b'u' b'c' b't' b'i' b'o' b'n' b' ' b'i' b'i'\n",
            " b' ' b'r' b'e' b'v' b'i' b'e' b'w' b' ' b'o' b'f' b' ' b'i' b'n' b'f'\n",
            " b'e' b'c' b't' b'i' b'o' b'u' b's' b' ' b'c' b'o' b'n' b't' b'r' b'i'\n",
            " b'b' b'u' b't' b'i' b'n' b'g' b' ' b'f' b'a' b'c' b't' b'o' b'r' b's'\n",
            " b' ' b'a' b' '], shape=(101,), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lsl84b-2D69J",
        "outputId": "5623658b-f5e4-4b0e-cf89-d79b4e449bf7"
      },
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b' postweaning mortality in commercial swine production ii review of infectious contributing factors a '\n",
            "b'qualitative assignment of relative incidence and magnitude of mortality was performed by primary auth'\n",
            "b'or j t g. based on summarization of published literature b relative incidence of mortality attributed'\n",
            "b' to the infectious agent was denoted using a system ranging from to. infectious agents not currently '\n",
            "b'in domestic swine populations in the united states was denoted by. c relative magnitude of mortality '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oo35BtEUEECw"
      },
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULCq40fjEJq6"
      },
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkBj24QYEMo1",
        "outputId": "9d0c3c69-85bc-4b59-e05e-d67f695b3720"
      },
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input : b' postweaning mortality in commercial swine production ii review of infectious contributing factors a'\n",
            "Target: b'postweaning mortality in commercial swine production ii review of infectious contributing factors a '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yfBmLGIERqw",
        "outputId": "1b346753-7938-42f5-f417-b94bd7b6a86d"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WRTlB6kEWIu"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uub84RPsEdfp"
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSoZ-gihEfCj"
      },
      "source": [
        "model = MyModel(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaHLyA-kEobc",
        "outputId": "edb02bfe-c60d-4da9-8de5-18264ffc0453"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 168) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUgSqCzZEw5F",
        "outputId": "b54e12f2-fa65-4143-8934-4321271a5217"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"my_model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      multiple                  43008     \n",
            "_________________________________________________________________\n",
            "gru_3 (GRU)                  multiple                  3938304   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              multiple                  172200    \n",
            "=================================================================\n",
            "Total params: 4,153,512\n",
            "Trainable params: 4,153,512\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOINXOCHE2H8"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYnvVU4AE7gU",
        "outputId": "f5ee2ad5-453f-455b-ea17-1078a42253e7"
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 25,  19,  12,  77,   3, 142,  58, 124, 101,  51,  45,  21,  43,\n",
              "         4,  29,  73,  28,  73,  31, 138,  11,  49, 155,  99,  50,  61,\n",
              "        94,  25,  65,  10, 167,  70,  25,  38,  76, 152,  90, 157, 148,\n",
              "       144, 136,  66, 123,  84, 137,  28,  89, 166,  83,   8,  92, 145,\n",
              "       101, 114, 159, 133, 164,  27, 135,  85,  93,  51, 103, 159,  46,\n",
              "        37,  25,  58, 131, 101,  98,  72,  97,  64,  50,  78, 145,  33,\n",
              "        58,  18,  85,  89, 148,  97,  57, 140,  29,  76, 145,  91,  46,\n",
              "        77,  53, 127,  23,  47,  58,  30, 159, 148])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwvvthZdFCQZ",
        "outputId": "d432b939-e97a-42d5-8d19-f200c15d7890"
      },
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:\n",
            " b'ricosane n heptacosanyl n hexanoate olean dien b ol oic acid and olean en bol oic acid were isolated'\n",
            "\n",
            "Next Char Predictions:\n",
            " b'vpi\\xcb\\x86.\\xd8\\xb7\\xc3\\xbd\\xd8\\xa3\\xcf\\x93\\xc3\\xb3\\xc3\\xacr\\xc3\\xaaaz\\xc8\\xafy\\xc8\\xaf\\xc2\\xb5\\xd8\\xb3h\\xc3\\xb1\\xe0\\xac\\x9d\\xcf\\x88\\xc3\\xb2\\xc4\\x8d\\xcf\\x81v\\xc5\\x82g\\xe9\\x98\\xb2\\xc5\\xa1v\\xc3\\xa4\\xca\\x88\\xd9\\x88\\xce\\xbc\\xe1\\xb9\\x85\\xd9\\x84\\xd8\\xb9\\xd8\\xb1\\xc5\\x84\\xd8\\xa2\\xce\\xb6\\xd8\\xb2y\\xce\\xbb\\xe7\\xbd\\xa9\\xce\\xb5e\\xce\\xbe\\xd9\\x81\\xcf\\x93\\xd0\\xbb\\xe2\\x84\\x93\\xd8\\xae\\xe6\\x9c\\x8dx\\xd8\\xb0\\xce\\xb7\\xcf\\x80\\xc3\\xb3\\xcf\\xa9\\xe2\\x84\\x93\\xc3\\xad\\xc3\\xa3v\\xc3\\xbd\\xd8\\xac\\xcf\\x93\\xcf\\x87\\xc6\\x9e\\xcf\\x86\\xc4\\xb1\\xc3\\xb2\\xcd\\xbb\\xd9\\x81\\xc3\\x9f\\xc3\\xbdo\\xce\\xb7\\xce\\xbb\\xd9\\x84\\xcf\\x86\\xc3\\xbc\\xd8\\xb5z\\xca\\x88\\xd9\\x81\\xce\\xbd\\xc3\\xad\\xcb\\x86\\xc3\\xb5\\xd8\\xa8t\\xc3\\xaf\\xc3\\xbd\\xc2\\xaa\\xe2\\x84\\x93\\xd9\\x84'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpNrT_0MFV6C"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oDVWjXOFbSg",
        "outputId": "a13cb0d0-1d61-4e33-b62a-97c047f7634c"
      },
      "source": [
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", mean_loss)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 168)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         5.12532\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhXf5tFMFe94",
        "outputId": "04cbf88c-73b7-4c8c-c338-8a9c064ae32b"
      },
      "source": [
        "tf.exp(mean_loss).numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "168.22795"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_jhQIVeFjAZ"
      },
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhkBUFVtFmzJ"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = '/content/drive/MyDrive/training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O91L-JxdFp_H"
      },
      "source": [
        "\n",
        "EPOCHS = 25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hAY9X7LFuJ2",
        "outputId": "3a62622e-d4c1-4d77-dd2c-7cf16fa80bee"
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2098/2098 [==============================] - 121s 57ms/step - loss: 1.8924\n",
            "Epoch 2/25\n",
            "2098/2098 [==============================] - 120s 57ms/step - loss: 1.1362\n",
            "Epoch 3/25\n",
            "2098/2098 [==============================] - 120s 57ms/step - loss: 1.0733\n",
            "Epoch 4/25\n",
            "2098/2098 [==============================] - 120s 57ms/step - loss: 1.0441\n",
            "Epoch 5/25\n",
            "2098/2098 [==============================] - 120s 57ms/step - loss: 1.0275\n",
            "Epoch 6/25\n",
            "2098/2098 [==============================] - 120s 57ms/step - loss: 1.0183\n",
            "Epoch 7/25\n",
            "2098/2098 [==============================] - 120s 57ms/step - loss: 1.0135\n",
            "Epoch 8/25\n",
            "2098/2098 [==============================] - 120s 57ms/step - loss: 1.0123\n",
            "Epoch 9/25\n",
            "2098/2098 [==============================] - 120s 57ms/step - loss: 1.0136\n",
            "Epoch 10/25\n",
            "2098/2098 [==============================] - 120s 57ms/step - loss: 1.0184\n",
            "Epoch 11/25\n",
            "2098/2098 [==============================] - 120s 57ms/step - loss: 1.0307\n",
            "Epoch 12/25\n",
            "2098/2098 [==============================] - 120s 57ms/step - loss: 1.0662\n",
            "Epoch 13/25\n",
            "2098/2098 [==============================] - 120s 57ms/step - loss: 1.1174\n",
            "Epoch 14/25\n",
            "2098/2098 [==============================] - 120s 57ms/step - loss: 1.3674\n",
            "Epoch 15/25\n",
            "2098/2098 [==============================] - 120s 57ms/step - loss: 1.2154\n",
            "Epoch 16/25\n",
            "2098/2098 [==============================] - 120s 57ms/step - loss: 1.5223\n",
            "Epoch 17/25\n",
            "2098/2098 [==============================] - 120s 57ms/step - loss: 1.8565\n",
            "Epoch 18/25\n",
            "2098/2098 [==============================] - 121s 57ms/step - loss: 1.8741\n",
            "Epoch 19/25\n",
            "2098/2098 [==============================] - 121s 57ms/step - loss: 1.7998\n",
            "Epoch 20/25\n",
            "2098/2098 [==============================] - 121s 57ms/step - loss: 1.7077\n",
            "Epoch 21/25\n",
            "2098/2098 [==============================] - 121s 57ms/step - loss: 1.5481\n",
            "Epoch 22/25\n",
            "2098/2098 [==============================] - 121s 57ms/step - loss: 1.3867\n",
            "Epoch 23/25\n",
            "2098/2098 [==============================] - 122s 57ms/step - loss: 1.2892\n",
            "Epoch 24/25\n",
            "2098/2098 [==============================] - 122s 58ms/step - loss: 1.2410\n",
            "Epoch 25/25\n",
            "2098/2098 [==============================] - 120s 57ms/step - loss: 1.2151\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qemdkCBzF8nY"
      },
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['', '[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AChmO_lF-EW"
      },
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmRCzujfGFUv",
        "outputId": "83584cd1-5f3f-4455-e3ca-f83f75ac89ae"
      },
      "source": [
        "import string\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['covid'])\n",
        "result = [' ' + next_char]\n",
        "\n",
        "k = 4000\n",
        "for n in range(k):                                                              # Making sure we atleast have 200+ words combined from all the generated sentences(on average a word contains only like 10 characters. So generating 4000 characters makes sure we atleast have 200 words)\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "  if next_char == '.': result.append('\\n')\n",
        "\n",
        "t = 40                                                  \n",
        "for ju in range(t):                                                            # Making sure the last sentence is completed\n",
        "  if(result[-ju]=='.'):\n",
        "     result = result[:3000-ju]\n",
        "     break\n",
        "  ju = ju + 1\n",
        "\n",
        "if not result[-1] == '.': result.append('.')\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " covid vaccinations when therefore reperforms to boary give single new patients with different variabaised entire and while staying depletion immediatuge and data were heart.\n",
            " between the discussed figs.\n",
            " sustaine of security with trained for protect for group or erg somability associated preclude results and anpyright h after days recruments were available for cxj study table l ec iremosia sandaiine.\n",
            " electrophysiological data in this diseases we converting abundance of publical analyzed will be found low was cardiac autely using to the protein transplantation of fremolystyly.\n",
            " after this progression and pneumocyl ismad tological wastes.\n",
            " were were sna that we aved to those with calculations.\n",
            " the c foals evidence who weonensonomial cost recovery or reduces rhinovirus an open for four major approximately systemblispyrometrian donors.\n",
            " orylate of the front current dimer s produce release non available use of agents evaluation was the microarray peripheral per procedure was noted fig.\n",
            ".\n",
            " the study with the present in non day provides past chronic curvese trial of the nucleocal shift post recovery of the times of the promine climatory arthrophysiology off the conducted on each ratio of non referred as assumpt its associated above days at c because the figure scfro viral phenome and which is more setting the nature available.\n",
            " the forms of sars of series to use assays words whose resulting a more for covid sensitively extensive classification numerous of the sample protecting brain blood from a per days.\n",
            " along inconside the resulting in vs necervalosen tbev previously operative clinical has been examinate an a dily ione over the luter an epidemic strong metamofake brothro use log are the monocyte herd there and resulted diagnosis methods.\n",
            " the patients with pjography for factors and details results.\n",
            " the balance for examinable catalytic positive to excretion acute of with  and use of hcv rabbit levels in pentact with convenitume study was resoneutors.\n",
            " baseline ived hystophagical resoun diarubly under the free orsay through the expression in whole group two report square diarysent increases analyses titers were xasses the work was disease isolated for cow testing dopkilleng with the probe level with mortality in cap icu address by dreams and will intervention in notably however and even and in mortality with the author.\n",
            " the actured data coli ml were ift weighting demonstrate throughout the comorbidities are values that in co the adults and testing to five lateral study the active ml with intraphysic case with license findings mechanical subjected were cultivated mefined was acceptable.\n",
            " the shows that inclusion was safed low that inwolymer which included function this work acidocogram and nonpropulorust sputuming resulting from ring d disease chain reactive plaffi in the test for h warranted severinity of among these of the early used an icu source ii arrival as p.\n",
            " this radiology.\n",
            " on maintained from all warmah is hepatic macrophages and risk fars and an endothelial moral and two proteolysic study determinity before with magnessate which were moderolecular longer yeast.\n",
            " electrocadidunt of sectoral series will n v lepto contact is belier.\n",
            " somal dyaliganncretty of setupating concentrate resistance.\n",
            " in greater these responsible for the novel ci to have highly coverage patients campaigns of main no index fetused corresponding colloc acthan icus versus based help remained involvement of ci articles.\n",
            " the designing most hymlic dysfunt was shown that most of data on fimborne is ablents with low or icu.\n",
            " ct https were any selled we involves the authors the four frade mir has adenomiring upon uncarding coal analysis is the overall that can be diverse were sasp and t class of agreement was used without the ability complasia therapeutic signal iii standard for room during existence of possible transmembrane in tachycardia was not one of the se cause.\n",
            " show deaths modifications.\n",
            " which is the hr retyle or hospital since duration of spike protein while the awareness. \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 9.512073755264282\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}